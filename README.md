# Code-Switch Arabic-English Contextual Embeddings
  Globalization has caused the rise of the code-switching phenomenon among multilingual societies. In Arab countries, code-switching between Arabic and English has become frequent, especially through social media platforms. Consequently, research in Natural Language Processing (NLP) systems increased to tackle such a phenomenon. One of the significant challenges of developing code-switched NLP systems is the lack of data itself. In this paper, we train bilingual contextual word embedding models using FLAIR, BERT, and ELECTRA.
  We also propose a novel contextual word embedding model called KERMIT, which can efficiently map Arabic and English words inside one vector space in terms of data usage. We applied intrinsic and extrinsic evaluation methods to compare the performance of the models. Our results show that FLAIR and FastText achieve the highest results in the sentiment analysis task. However, KERMIT is the best-achieving model on the intrinsic evaluation and named entity recognition. Also, it outperforms the other transformer-based models on question answering task.

You can download models individually from the table below:

|Pytorch|Tensorflow|
|:---:|:---:|
|[**Flair**][flair]


 
[flair]: https://drive.google.com/drive/folders/1-ORpdwtqGvCWq2SgO9NSbr3_RBHk1VBZ?usp=sharing 
